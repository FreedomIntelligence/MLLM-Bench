| **Models**                        | **# Params** | **Open-sourced** | **Visual Adapter** | **Base LLM**    |
|-----------------------------------|--------------|------------------|--------------------|-----------------|
| GPT-4V                            | /            | no               | /                  | /               |
| CogVLM-Chat                       | 17.6B        | yes              | EVAV2-CLIP-E       | Vicuna-7B-v1.5  |
| LLaVA-v1.5-13B                    | 13B          | yes              | CLIP-ViT-L         | Vicuna-13B-v1.5 |
| InstructBLIP-Vicuna-13B           | 14B          | yes              | ViT+Q-Former       | Vicuna-13B      |
| Qwen-VL-Chat                      | 9.6B         | yes              | ViT-bigG           | Qwen-7B         |
| mPLUG-Owl2-LLaMA2-7B              | 8B           | yes              | ViT-L              | LLaMA2-7B       |
| SEED-LLaMA                        | 13B          | yes              | Multiple           | LLaMA2-13B      |
| kosmos-2-patch14-224              | 1.7B         | yes              | /                  | Magneto         |
| IDEFICS-9B-instruct               | 9B           | yes              | OpenCLIP           | LLaMA-7B        |
| Fuyu-8B                           | 9.4B         | yes              | Linear             | Persimmon       |
| MiniGPT-v2                        | 7.8B         | yes              | EVA                | LLaMA2-7B       |
| LVIS-Instruct4v-LLaVA-7b          | 7B           | yes              | CLIP-ViT-L         | Vicuna-7B-v1.5  |
